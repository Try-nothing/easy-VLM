import argparse
import os
import random
import numpy as np
import torch
import warnings
from PIL import Image
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from model.model_vlm import MiniMindVLM, VLMConfig

warnings.filterwarnings('ignore')

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def init_model(
        lm_config, 
        device, 
):
    tokenizer = AutoTokenizer.from_pretrained('./model')
    if args.load == 0: # å½“load==0æ˜¯ä½¿ç”¨pytorchå¯¼å…¥æ¨¡å‹
        moe_path = '_moe' if args.use_moe else ""
        modes = {0: "pretrain_vlm", 1: "sft_vlm", 2: "sft_vlm_multi"}
        ckp = f"./{args.out_dir}/{modes[args.model_mode]}_{args.hidden_size}{moe_path}.pth"
        model = MiniMindVLM(
            lm_config, 
            vision_model_path="./model/vision_model/clip-vit-base-patch16"
        )
        state_dict = torch.load(ckp, map_location=device)
        model.load_state_dict(
            {k:v for k,v in state_dict.item() if "mask" not in k}, 
            strict=False
        )

    else:
        # ç”¨transformersåº“åŠ è½½æ¨¡å‹
        transformers_model_path = "MiniMind2-Small-V"
        tokenizer = AutoTokenizer.from_pretrained(transformers_model_path)
        model = AutoModelForCausalLM.from_pretrained(transformers_model_path, trust_remote_code=True)
        model.vision_encoder, model.processor = MiniMindVLM.get_vision_model("./model/vision_model/clip-vit-base-patch16")

    print(f"VLMå‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} ç™¾ä¸‡")

    vision_model, preprocess = model.vision_encoder, model.processor
    return model.eval().to(device), tokenizer, vision_model.eval().to(device), preprocess

def setup_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True 
    torch.backends.cudnn.benchmark = False 

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--lora_name", default="None", type=str)
    parser.add_argument("--out_dir", default="out", type=str)
    parser.add_argument("--temperature", default=0.65, type=float)
    parser.add_argument("--top_p", default=0.85, type=float)
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu", type=str)

    # small model: hidden_size=512, num_hidden_layers=8
    # mid model: hidden_size=768, num_hidden_layers=16
    parser.add_argument("--hidden_size", default=512, type=int)
    parser.add_argument("--num_hidden_layers", default=8, type=int)
    parser.add_argument("--max_seq_len", default=8192, type=int)
    parser.add_argument("--use_moe", default=False, type=bool)

    parser.add_argument("--use_multi", default=1, type=int) # 1ä¸ºå•å›¾æ¨ç†ï¼Œ2ä¸ºå¤šå›¾æ¨ç†
    parser.add_argument("--stream", default=True, type=bool) # æ˜¯å¦è¿›è¡Œæµå¼è¾“å‡ºã€‚æµå¼è¾“å‡ºä¼šåŠ¨æ€çš„è¾“å‡ºæ¨¡å‹ç”Ÿæˆçš„ç»“æœã€‚éæµå¼è¾“å‡ºä¼šåœ¨æ¨¡å‹ç”Ÿæˆå®Œæ•´çš„å†…å®¹åä¸€æ¬¡æ€§çš„è¾“å‡ºã€‚
    parser.add_argument("--load", default=1, type=int, help="0: åŸç”Ÿtorchæƒé‡ï¼Œ1: transformersåŠ è½½")
    parser.add_argument("--model_mode", default=1, type=int, help="0: Pretrainæ¨¡å‹ï¼Œ1: SFTæ¨¡å‹ï¼Œ2: SFT-å¤šå›¾æ¨¡å‹ (betaæ‹“å±•)")
    args = parser.parse_args()

    # å¯¼å…¥æ¨¡å‹
    # é¦–å…ˆè®¾ç½®æ¨¡å‹çš„å‚æ•°
    # ç„¶åå†ç”¨init_modelå¯¼å…¥æ¨¡å‹
    lm_config = VLMConfig(
        hidden_size=args.hidden_size, 
        num_hidden_layers=args.num_hidden_layers, 
        max_seq_len=args.max_seq_len, 
        use_moe=args.use_moe
    )
    model, tokenizer, vision_model, preprocess = init_model(lm_config, args.device)
    # åˆ›å»ºæ–‡æœ¬æµç”Ÿæˆå™¨
    # å°†æ¨¡å‹ç”Ÿæˆçš„æ–‡å­—å®æ—¶æ˜¾ç¤ºåœ¨å±å¹•ä¸Š
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    def chat_with_vlm(
            prompt, 
            pixel_values, 
            image_names, 
    ):
        """
        è¾“å…¥promptå’Œå›¾åƒï¼Œè¾“å‡ºæ¨¡å‹ç”Ÿæˆçš„æ–‡å­—
        """
        # ç”Ÿæˆprompt
        messages = [{"role": "user", "content": prompt}]
        new_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )[-args.max_seq_len+1:]

        inputs = tokenizer(
            new_prompt, 
            return_tensors="pt", 
            trunction=True, 
        ).to(args.device)

        print(f"[Image]: {image_names}")
        print('ğŸ¤–ï¸: ', end='')
        generated_ids = model.generate(
            inputs["input_ids"],
            max_new_tokens=args.max_seq_len, 
            num_return_sequences=1,  # åªç”Ÿæˆä¸€ä¸ªå›ç­”
            do_sample=True, # å¯ç”¨éšæœºé‡‡æ ·ï¼Œå¢åŠ ç»“æœçš„å¤šæ ·æ€§
            attention_mask=inputs["attention_mask"], 
            pad_token_id=tokenizer.eos_token_id, 
            eos_token_id=tokenizer.eos_token_id, 
            streamer=streamer, # æµå¼è¾“å‡º
            top_p=args.top_p, # é‡‡æ ·å‚æ•°
            temperature=args.temperature, # æ¸©åº¦å‚æ•°
            pixel_values=pixel_values, # å›¾åƒç‰¹å¾
        )

        # å°†ç”Ÿæˆçš„å›ç­”è§£ç ä¸ºæ–‡æœ¬
        response = tokenizer.decode(
            generated_ids[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True
        )
        messages.append({
            "role": "assistant", 
            "content": response
        })
        print("\n\n")

    if args.use_multi == 1:
        image_dir = "./dataset/eval_images"
        prompt = f"{model.params.image_special_token}\næè¿°ä¸€ä¸‹è¿™ä¸ªå›¾åƒçš„å†…å®¹ã€‚"

        for image_file in os.listdir(image_dir):
            # å¯¼å…¥å›¾åƒ
            image = Image.open(os.path.join(image_dir, image_file)).convert("RGB")
            # å°†å›¾åƒè½¬åŒ–ä¸ºtensor
            pixel_tensors = MiniMindVLM.image2tensor(image, preprocess).to(args.device).unsqueeze(0)
            # è°ƒç”¨æ¨¡å‹
            chat_with_vlm(prompt, pixel_tensors, image_file)

    if args.use_multi == 2:
        args.model_mode = 2
        image_dir = "./dataset/eval_images/bird/"
        prompt = (
            f"{lm_config.image_special_token}\n"
            f"{lm_config.image_special_token}\n"
            f"æ¯”è¾ƒä¸€ä¸‹è¿™ä¸¤å¼ å›¾ç‰‡ï¼Œå®ƒä»¬æœ‰ä»€ä¹ˆå¼‚åŒç‚¹ï¼Ÿ"
        )
        pixel_tensors_multi = []
        for image_file in os.listdir(image_dir):
            image = Image.open(os.path.join(image_dir, image_file)).convert("RGB")
            pixel_tensors_multi.append(MiniMindVLM.image2tensor(image, preprocess))

        pixel_tensors = torch.cat(pixel_tensors_multi, dim=0).to(args.device).unsqueeze(0)

        for _ in range(10):
            chat_with_vlm(prompt, pixel_tensors, (', '.join(os.listdir(image_dir))))